{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: process delay scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import karabo_data as kd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from time import strftime\n",
    "import os\n",
    "import dssc_process as dp\n",
    "\n",
    "kd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure subfolders exist\n",
    "for f in ['tmp', 'images', 'processed_runs']:\n",
    "    if not os.path.isdir(f):\n",
    "        os.mkdir(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup processing and index non-DSSC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 145 ms, sys: 46.1 ms, total: 191 ms\n",
      "Wall time: 544 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# basic run information\n",
    "proposal = 2212\n",
    "run_nr = 235\n",
    "is_dark = False\n",
    "\n",
    "# DSSC frame names - make sure, \"dark\" is included in names for frames without FEL\n",
    "framepattern = ['pumped', 'unpumped']\n",
    "# framepattern = ['pumped', 'pumped_dark', 'unpumped', 'unpumped_dark']\n",
    "# framepattern = ['image']\n",
    "\n",
    "maxframes = None  # set to limit the number of frames per train\n",
    "# maxrames = 148\n",
    "\n",
    "# scan settings (set scan_variable to None for static data)\n",
    "stepsize = .03  # set to None to disable rounding - might give poor statistics on some values\n",
    "\n",
    "# scan_variable = ('SCS_ILH_LAS/PHASESHIFTER/DOOCS', 'actualPosition.value')\n",
    "scan_variable = ('SCS_ILH_LAS/DOOCS/PP800_PHASESHIFTER', 'actualPosition.value')\n",
    "# scan_variable = ('SA3_XTD10_MONO/MDL/PHOTON_ENERGY', 'actualEnergy.value')\n",
    "# scan_variable = ('SCS_ILH_LAS/DOOCS/PPL_OPT_DELAY', 'actualPosition.value')\n",
    "\n",
    "scan_variable = None if is_dark else scan_variable\n",
    "\n",
    "# index non-DSSC data\n",
    "run = kd.open_run(proposal, run_nr, include='*DA*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare scan variable and write to file for subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSSC frames per train: 20\n"
     ]
    }
   ],
   "source": [
    "scanfile = './tmp/scan.h5'\n",
    "maskfile = './tmp/mask.h5'\n",
    "# maskfile = None\n",
    "\n",
    "for fname in [scanfile, maskfile]:\n",
    "    if fname is not None:\n",
    "        if os.path.isfile(fname):\n",
    "            os.remove(fname)\n",
    "\n",
    "dssc_info = dp.load_dssc_info(proposal, run_nr)\n",
    "fpt = dssc_info['frames_per_train']\n",
    "print('DSSC frames per train:', fpt)\n",
    "\n",
    "scan = dp.load_scan_variable(run, scan_variable, stepsize)\n",
    "scan.to_netcdf(scanfile, group='data', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional: discard groups with low number of trains\n",
    "This is usually not necessary when stepsize is set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 10\n",
    "\n",
    "grouped = scan.groupby(scan)\n",
    "\n",
    "for val, grp in grouped:\n",
    "    if len(grp) < 10:\n",
    "        scan.loc[{'trainId': grp.trainId.values}] = np.nan\n",
    "\n",
    "scan = scan.dropna('trainId')\n",
    "scan.to_netcdf(scanfile, group='data', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask individual pulses based on XGM thresholding\n",
    "It is possible to pass a 2d binary mask (DataArray dimensions 'trainId' and 'pulse) to (de)select trains and/or pulses.\n",
    "Broadcasting is supported, so passing just a 1d DataArray with one of the two dimensions is also possible.\n",
    "Care has to be taken to match the frame numbers of the DSSC data - especially when intermediate dark frames are recorded, the number of XGM pulse numbers do not necessarily match the DSSC frame numbers.\n",
    "This functionality may also be used to limit the number of DSSC frames processed, e.g., when more frames than FEL pulses were recorded (take care that dark runs (i.e., possibly without any XGM data) still get a correct mask in that case!).\n",
    "\n",
    "The following example selects based on XGM threshold, but you can of course build your own selection mask based on any other information as well. Just save the final xarray.DataArray to \"maskfile\" as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASE3 bunches per train: 20\n",
      "rejecting 249 out of 180180 pulses (0.1%) due to xgm threshold\n"
     ]
    }
   ],
   "source": [
    "xgm_min = 0\n",
    "xgm_max = np.inf\n",
    "\n",
    "if not is_dark:\n",
    "    xgm = dp.load_xgm(run, print_info=True)\n",
    "    xgm_frame_coords = dp.calc_xgm_frame_indices(xgm.shape[1], framepattern)\n",
    "    xgm['pulse'] = xgm_frame_coords\n",
    "    \n",
    "if maskfile is not None:\n",
    "    # default mask - all pulses and trains included\n",
    "    pulsemask = xr.DataArray(np.ones([len(run.train_ids), fpt], dtype=bool),\n",
    "                             dims=['trainId', 'pulse'],\n",
    "                             coords={'trainId': run.train_ids, 'pulse': range(fpt)})\n",
    "\n",
    "    if not is_dark:\n",
    "        n_frames_dark = len([p for p in framepattern if 'dark' in p])\n",
    "        valid = (xgm > xgm_min) * (xgm < xgm_max)\n",
    "        pulsemask = valid.combine_first(pulsemask).astype(bool)\n",
    "        nrejected = int(valid.size - valid.sum())\n",
    "        percent_rejected = 100 * nrejected / valid.size\n",
    "        print(f'rejecting {nrejected} out of {valid.size} pulses ({percent_rejected:.1f}%) due to xgm threshold')\n",
    "    pulsemask.to_netcdf(maskfile, group='data', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot XGM and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94bbf5a4e824d1f9191320c5d65f31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "# ax1.plot(scan.xgm.mean('dim_0'), label='pumped')\n",
    "ax1.plot(xgm.trainId, xgm, 'o', c='C0', ms=1)\n",
    "ax1.set_ylabel('xgm')\n",
    "if maskfile is not None:\n",
    "    ax1.axhline(xgm_min, c='r')\n",
    "    ax1.axhline(xgm_max, c='r')\n",
    "\n",
    "ax2.plot(scan.trainId, scan)\n",
    "ax2.set_ylabel('scan variable')\n",
    "ax2.set_xlabel('trainId')\n",
    "\n",
    "ax1.set_title(f'run: {run_nr}')\n",
    "\n",
    "tstamp = strftime('%y%m%d_%H%M')\n",
    "# fig.savefig(f'images/run{run_nr}_scan_{tstamp}.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot number of trains per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d3531ff2d24330976fff9808be2b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = xr.DataArray(np.ones(len(scan)),\n",
    "                      dims=['scan_variable'],\n",
    "                      coords={'scan_variable': scan.values},\n",
    "                      name='counts')\n",
    "\n",
    "counts = counts.groupby('scan_variable').sum()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(counts.scan_variable, counts, 'o', ms=4)\n",
    "ax.set_xlabel('scan variable')\n",
    "ax.set_ylabel('number of trains')\n",
    "ax.set_title(f'run {run_nr}')\n",
    "ax.grid(True)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create joblist for multiprocessing\n",
    "This is a conservative estimate for the maximum number of trains to process simultaneously without using more than \"max_GB\" gigabytes of memory. Caps out at 512 trains per chunk, as there doesn't seem to be any performance benefit beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 512 trains per chunk\n"
     ]
    }
   ],
   "source": [
    "max_GB = 400\n",
    "\n",
    "# max_GB / (8byte * 16modules * 128px * 512px * N_pulses)\n",
    "chunksize = int(max_GB * 128 // fpt)\n",
    "chunksize = min(512, chunksize)  # more than 512 trains doesn't seem to give any performance benefit\n",
    "print('processing', chunksize, 'trains per chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for m in range(16):\n",
    "    jobs.append(dict(\n",
    "        proposal=proposal,\n",
    "        run_nr=run_nr,\n",
    "        module=m,\n",
    "        chunksize=chunksize,\n",
    "        scanfile=scanfile,\n",
    "        framepattern=framepattern,\n",
    "        maskfile=None if is_dark else maskfile,\n",
    "        maxframes=maxframes,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create multiprocessing pool and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 09:32:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [17:15<00:00, 55.16s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'start time:', strftime('%X'))\n",
    "\n",
    "with multiprocessing.Pool(16) as pool:\n",
    "    module_data = pool.map(dp.process_dssc_module, jobs)\n",
    "    \n",
    "print('finished:', strftime('%X'))\n",
    "\n",
    "module_data = xr.concat(module_data, dim='module')\n",
    "module_data = module_data.dropna('scan_variable')\n",
    "module_data['run'] = run_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge processed data with scan variable and normalization data\n",
    "If trains/ pulses were filtered, this is a good place to add additional data that has to be filtered in the same way (e.g., TIM). Just use the same binary mask (\"valid\") before grouping and adding it to the processed DSSC data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional: load TIM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = dp.load_TIM(run)\n",
    "tim['pulse'] = xgm_frame_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973a5c03405f4d279bf163a9587af88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(xgm[:, 0], tim[:, 0], 'o', ms=2, label='pumped')\n",
    "ax.plot(xgm[:, 1], tim[:, 1], 'o', ms=2, label='unpumped')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(f'run {run_nr}')\n",
    "ax.set_xlabel('XGM')\n",
    "ax.set_ylabel('TIM (MCP2apd)')\n",
    "\n",
    "tstamp = strftime('%y%m%d_%H%M')\n",
    "fig.savefig(f'images/run{run_nr}_TIM_{tstamp}.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge with module data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_dark:\n",
    "    pulses_no_dark = [p for p in framepattern if 'dark' not in p]\n",
    "    \n",
    "    if maskfile is not None:\n",
    "        xgm = xgm.where(valid)  # IMPORTANT: use the same mask for normalization data!\n",
    "    xgm = dp.split_frames(xgm, pulses_no_dark, prefix='xgm_')\n",
    "    xgm['scan_variable'] = scan\n",
    "    xgm = xgm.groupby('scan_variable').mean('trainId')\n",
    "    module_data = xr.merge([module_data, xgm])\n",
    "    \n",
    "#     if maskfile is not None:\n",
    "#         tim = tim.where(valid)\n",
    "#     tim = dp.split_frames(tim, pulses_no_dark, prefix='tim_')\n",
    "#     tim['scan_variable'] = scan\n",
    "#     tim = tim.groupby('scan_variable').mean('trainId')\n",
    "#     module_data = xr.merge([module_data, tim])\n",
    "    \n",
    "module_data = module_data.transpose('scan_variable', 'module', 'x', 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving:  ./processed_runs/run235_by-delay.h5\n"
     ]
    }
   ],
   "source": [
    "prefix = ''\n",
    "overwrite = True\n",
    "\n",
    "save_folder = './processed_runs/'\n",
    "\n",
    "if is_dark:\n",
    "    fname = f'{prefix}run{run_nr}.h5'  # no scan\n",
    "else:\n",
    "    fname = f'{prefix}run{run_nr}_by-delay.h5'  # run with delay scan (change for other scan types!)\n",
    "\n",
    "\n",
    "save_path = os.path.join(save_folder, fname)\n",
    "file_exists = os.path.isfile(save_path)\n",
    "\n",
    "if (not file_exists) or (file_exists and overwrite):\n",
    "    if file_exists:\n",
    "        os.remove(save_path)\n",
    "    module_data.to_netcdf(save_path, group='data')\n",
    "    os.chmod(os.path.join(save_folder, fname), 664)\n",
    "    print('saving: ', save_path)\n",
    "else:\n",
    "    print('file', save_path, 'exists and overwrite is False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
