{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import karabo_data as kd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from time import strftime\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm, BoundaryNorm\n",
    "from importlib import reload\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from glob import glob\n",
    "from karabo_data.read_machinery import find_proposal\n",
    "\n",
    "kd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure subfolders exist\n",
    "for f in ['tmp', 'images', 'processed_runs']:\n",
    "    if not os.path.isdir(f):\n",
    "        os.mkdir(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# worker function for multiprocessing\n",
    "is saved to file \"multiprocess_defs_v3.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting multiprocess_defs_v4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile multiprocess_defs_v4.py\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "@author: Michael Schneider, with input from XFEL-CAS and SCS beamline staff\n",
    "\n",
    "Created on October 2, 2019\n",
    "October 10, 2019: added pulse- and train-based masking\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import karabo_data as kd\n",
    "from karabo_data.read_machinery import find_proposal\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from time import strftime, sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def find_run_dir(proposal, run):\n",
    "    '''returns the raw data folder for given (integer) proposal and run number'''\n",
    "    proposal_dir = find_proposal(f'p{proposal:06d}')\n",
    "    return os.path.join(proposal_dir, f'raw/r{run:04d}')\n",
    "\n",
    "\n",
    "def load_run_selective(proposal, run_nr, include=None, exclude=None, maxfiles=None):\n",
    "    '''\n",
    "    Finds the rundirectory for the given (integer) proposal and run numbers and returns\n",
    "    a karabo_data.DataCollection instance. Files can be selected or excluded based on\n",
    "    filenames. Setting any of the optional paramters to None disables that particular\n",
    "    selection.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    proposal :: int\n",
    "        proposal number\n",
    "    run_nr :: int\n",
    "        run number\n",
    "    include :: str\n",
    "        Load only files that have this string in the filename\n",
    "    exclude :: str\n",
    "        Skip files with this string in the filename\n",
    "    maxfiles :: int\n",
    "        Load at most this many files\n",
    "    '''\n",
    "    rundir = find_run_dir(proposal, run_nr)\n",
    "    flist = glob(os.path.join(rundir, '*h5'))\n",
    "\n",
    "    if include is not None:\n",
    "        flist = [f for f in flist if include in f]\n",
    "\n",
    "    if exclude is not None:\n",
    "        flist = [f for f in flist if exclude not in f]\n",
    "    \n",
    "    return kd.DataCollection.from_paths(flist[:maxfiles])\n",
    "\n",
    "\n",
    "def load_scan_variable(run, scan_variable, stepsize=None):\n",
    "    '''\n",
    "    Loads the given scan variable and rounds scan positions to integer multiples of \"stepsize\"\n",
    "    for consistent grouping. Creates a dummy scan if scan_variable is set to None.\n",
    "    Parameters:\n",
    "        run : (karabo_data.DataCollection) RunDirectory instance\n",
    "        scan_variable : (tuple of str) (\"source name\", \"value path\"), examples:\n",
    "                        ('SCS_ILH_LAS/PHASESHIFTER/DOOCS', 'actualPosition.value')\n",
    "                        ('SCS_ILH_LAS/DOOCS/PPL_OPT_DELAY', 'actualPosition.value')\n",
    "                        ('SA3_XTD10_MONO/MDL/PHOTON_ENERGY', 'actualEnergy.value')\n",
    "                        None creates a dummy file to average over all trains of the run\n",
    "        stepsize : (float) nominal stepsize of the scan - values of scan_variable will be\n",
    "                   rounded to integer multiples of this value\n",
    "    '''\n",
    "    if scan_variable is not None:\n",
    "        source, path = scan_variable\n",
    "        scan = run.get_array(source, path)\n",
    "        if stepsize is not None:\n",
    "            scan = stepsize * np.round(scan / stepsize)\n",
    "    else:\n",
    "        # dummy scan variable - this will average over all trains\n",
    "        scan = xr.DataArray(np.ones(len(run.train_ids), dtype=np.int16),\n",
    "                            dims=['trainId'], coords={'trainId': run.train_ids})\n",
    "    scan.name = 'scan_variable'\n",
    "    return scan\n",
    "\n",
    "\n",
    "def load_xgm(run, print_info=False):\n",
    "    '''Returns the XGM data from loaded karabo_data.DataCollection'''\n",
    "    nbunches = run.get_array('SCS_RR_UTC/MDL/BUNCH_DECODER', 'sase3.nPulses.value')\n",
    "    nbunches = np.unique(nbunches)\n",
    "    if len(nbunches) == 1:\n",
    "        nbunches = nbunches[0]\n",
    "    else:\n",
    "        warnings.warn('not all trains have same length DSSC data')\n",
    "        print('nbunches: ', nbunches)\n",
    "        nbunches = max(nbunches)\n",
    "    if print_info:\n",
    "        print('SASE3 bunches per train:', nbunches)\n",
    "    \n",
    "    xgm = run.get_array('SCS_BLU_XGM/XGM/DOOCS:output', 'data.intensitySa3TD',\n",
    "                        roi=kd.by_index[:nbunches], extra_dims=['pulse'])\n",
    "    return xgm\n",
    "\n",
    "\n",
    "def load_TIM(run, apd='MCP2apd'):\n",
    "    '''\n",
    "    Load TIM traces and match them to SASE3 pulses. \"run\" is a karabo_data.RunDirectory instance.\n",
    "    '''\n",
    "    import ToolBox as tb\n",
    "    \n",
    "    fields = [\"sase1\", \"sase3\", \"npulses_sase3\", \"npulses_sase1\", apd, \"SCS_SA3\", \"nrj\"]\n",
    "    timdata = xr.Dataset()\n",
    "    for f in fields:\n",
    "        m = tb.mnemonics[f]\n",
    "        timdata[f] = run.get_array(m['source'], m['key'], extra_dims=m['dim'])\n",
    "\n",
    "    timdata.attrs['run'] = run\n",
    "    timdata = tb.matchXgmTimPulseId(timdata)\n",
    "    return timdata.rename({'sa3_pId': 'pulse'})[apd]\n",
    "\n",
    "\n",
    "def prepare_module_empty(scan_variable, framepattern):\n",
    "    '''Create empty (zero-valued) DataArray for a single DSSC module to iteratively add data to'''\n",
    "    len_scan = len(np.unique(scan_variable))\n",
    "    dims = ['scan_variable', 'x', 'y']\n",
    "    coords = {'scan_variable': np.unique(scan_variable)}\n",
    "    shape = [len_scan, 128, 512]\n",
    "        \n",
    "    empty = xr.DataArray(np.zeros(shape, dtype=float), dims=dims, coords=coords)\n",
    "    module_data = xr.Dataset()\n",
    "    for name in framepattern:\n",
    "        module_data[name] = empty.copy()\n",
    "    \n",
    "    module_data['sum_count'] = xr.DataArray(np.zeros(len_scan, dtype=int), dims=['scan_variable'])\n",
    "    return module_data\n",
    "\n",
    "\n",
    "def load_dssc_info(proposal, run_nr):\n",
    "    '''Loads the first data file for DSSC module 0 (this is hardcoded) and\n",
    "    returns the detector_info dictionary'''\n",
    "    dssc_module = load_run_selective(proposal, run_nr, include='DSSC00', maxfiles=1)\n",
    "    dssc_info = dssc_module.detector_info('SCS_DET_DSSC1M-1/DET/0CH0:xtdf')\n",
    "    return dssc_info\n",
    "\n",
    "\n",
    "def load_chunk_data(sel, sourcename):\n",
    "    '''Load DSSC data (sel is a DataCollection or a subset of a DataCollection\n",
    "    obtained by its select_trains() method). The flattened multi-index (trains+pulses)\n",
    "    is unraveled before returning the data.\n",
    "    '''\n",
    "    fpt = sel.detector_info(sourcename)['frames_per_train']\n",
    "    data = sel.get_array(sourcename, 'image.data', extra_dims=['_empty_', 'x', 'y']).squeeze()\n",
    "    \n",
    "    tids = np.unique(data.trainId)\n",
    "    data = data.rename(dict(trainId='trainId_pulse'))\n",
    "    \n",
    "    midx = pd.MultiIndex.from_product([sorted(tids), range(fpt)], names=('trainId', 'pulse'))\n",
    "    data = xr.DataArray(data, dict(trainId_pulse=midx)).unstack('trainId_pulse')\n",
    "    data = data.transpose('trainId', 'pulse', 'x', 'y')\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_chunk_data(module_data, chunk_data, framepattern):\n",
    "    '''Merge chunk data with prepared dataset for entire module.\n",
    "    Aligns on \"scan_variable\" and sums values for variables\n",
    "    ['pumped', 'unpumped', 'sum_count']'''\n",
    "    where = dict(scan_variable=chunk_data.scan_variable)\n",
    "    for var in framepattern + ['sum_count']:\n",
    "        # module_data[var].loc[where] = module_data[var].loc[where] + chunk_data[var]\n",
    "        # previous line doesn't convert to larger dtypes when necessary\n",
    "        # the next line concatenates the data along a new dimension ('tmp') and uses\n",
    "        # the sum() method, which supports automatic conversion\n",
    "        summed = xr.concat([module_data[var].loc[where], chunk_data[var]], dim='tmp').sum('tmp')\n",
    "        module_data[var].loc[where] = summed\n",
    "    return module_data\n",
    "\n",
    "\n",
    "def split_frames(data, pattern, prefix=''):\n",
    "    '''Split frames according to \"pattern\" and average over resulting splits.\n",
    "    \"pattern\" is a list of frame names (order matters!). Examples:\n",
    "        pattern = ['pumped', 'pumped_dark', 'unpumped', 'unpumped_dark']  # 4 DSSC frames, 2 FEL pulses\n",
    "        pattern = ['pumped', 'unpumped']  # 2 FEL frames, no intermediate darks\n",
    "        pattern = ['image']  # no splitting, average over all frames\n",
    "    Returns a dataset with data variables named prefix + framename\n",
    "    '''\n",
    "    n = len(pattern)\n",
    "    dataset = xr.Dataset()\n",
    "    for i, name in enumerate(pattern):\n",
    "        dataset[prefix + name] = data.loc[{'pulse': np.s_[i::n]}].mean('pulse')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def calc_xgm_frame_indices(nbunches, framepattern):\n",
    "    '''\n",
    "    Returns a coordinate array for XGM data. The coordinates correspond to DSSC\n",
    "    frame numbers and depend on the number of FEL pulses per train (\"nbunches\")\n",
    "    and the framepattern. In framepattern, dark DSSC frame names (i.e., without\n",
    "    FEL pulse) _must_ include \"dark\" as a substring.\n",
    "    '''\n",
    "    n_frames = len(framepattern)\n",
    "    n_data_frames = np.sum(['dark' not in p for p in framepattern])\n",
    "    frame_max = nbunches * n_frames // n_data_frames\n",
    "\n",
    "    frame_indices = []\n",
    "    for i, p in enumerate(framepattern):\n",
    "        if 'dark' not in p:\n",
    "            frame_indices.append(np.arange(i, frame_max, n_frames))\n",
    "\n",
    "    return np.sort(np.concatenate(frame_indices))\n",
    "\n",
    "\n",
    "def process_dssc_module(job):\n",
    "    '''Aggregate DSSC data (chunked, to fit into memory) for a single module.\n",
    "    Groups by \"scan_variable\" in given scanfile - use dummy scan_variable to average\n",
    "    over all trains. This implies, that only trains found in the scanfile are considered.\n",
    "    Designed for the multiprocessing module - expects a job dictionary with the following keys:\n",
    "      proposal : (int) proposal number\n",
    "      run : (int) run number\n",
    "      module : (int) DSSC module to process\n",
    "      chunksize : (int) number of trains to process simultaneously\n",
    "      scanfile : (str) name of hdf5 file with xarray.DataArray containing the scan variable and trainIds\n",
    "      framepattern : (list of str) names for the (possibly repeating) intra-train pulses. See split_dssc_data\n",
    "      pulsemask : (str) name of hdf5 file with boolean xarray.DataArray to select/reject trains and pulses\n",
    "    '''\n",
    "    proposal = job['proposal']\n",
    "    run_nr = job['run_nr']\n",
    "    module = job['module']\n",
    "    chunksize = job['chunksize']\n",
    "    scanfile = job['scanfile']\n",
    "    framepattern = job['framepattern']\n",
    "    maskfile = job.get('maskfile', None)\n",
    "    \n",
    "    sourcename = f'SCS_DET_DSSC1M-1/DET/{module}CH0:xtdf'\n",
    "    \n",
    "    collection = load_run_selective(proposal, run_nr, include=f'DSSC{module:02d}')\n",
    "        \n",
    "    ntrains = len(collection.train_ids)\n",
    "    \n",
    "    # read preprocessed scan variable from file - selection and (possibly) rounding already done.\n",
    "    scan = xr.open_dataarray(scanfile, 'data', autoclose=True)\n",
    "\n",
    "    # read binary pulse/train mask - e.g. from XGM thresholding\n",
    "    if maskfile is not None:\n",
    "        pulsemask = xr.open_dataarray(maskfile, 'data', autoclose=True)\n",
    "    else:\n",
    "        pulsemask = None\n",
    "    \n",
    "    module_data = prepare_module_empty(scan, framepattern)\n",
    "    chunks = np.arange(ntrains, step=chunksize)\n",
    "    if module == 15:\n",
    "        pbar = tqdm(total=len(chunks) + 1)\n",
    "    for start_index in chunks:\n",
    "        sel = collection.select_trains(kd.by_index[start_index:start_index + chunksize])\n",
    "        data = load_chunk_data(sel, sourcename)\n",
    "        if pulsemask is not None:\n",
    "            data = data.where(pulsemask)\n",
    "\n",
    "        data = split_frames(data, framepattern)\n",
    "        data['sum_count'] = xr.full_like(data.trainId, fill_value=1)\n",
    "        data['scan_variable'] = scan  # aligns on trainId, drops non-matching trains \n",
    "        data = data.groupby('scan_variable').sum('trainId')\n",
    "        module_data = merge_chunk_data(module_data, data, framepattern)\n",
    "        if module == 15:\n",
    "            pbar.update(1)\n",
    "    \n",
    "    for name in framepattern:\n",
    "        module_data[name] = module_data[name] / module_data.sum_count\n",
    "    if module == 15:\n",
    "            pbar.update(1)\n",
    "    return module_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess_defs_v4 as process\n",
    "\n",
    "process = reload(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup processing and index non-DSSC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 459 ms, sys: 165 ms, total: 624 ms\n",
      "Wall time: 627 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# basic run information\n",
    "proposal = 2280\n",
    "run_nr = 132\n",
    "is_dark = False\n",
    "\n",
    "# DSSC frame names - make sure, \"dark\" is included in names for frames without FEL\n",
    "framepattern = ['unpumped', 'unpumped_dark', 'pumped', 'pumped_dark']\n",
    "\n",
    "# scan settings (set scan_variable to None for static data)\n",
    "stepsize = None\n",
    "# scan_variable = ('SCS_ILH_LAS/PHASESHIFTER/DOOCS', 'actualPosition.value')\n",
    "# scan_variable = ('SA3_XTD10_MONO/MDL/PHOTON_ENERGY', 'actualEnergy.value')\n",
    "scan_variable = ('SCS_ILH_LAS/DOOCS/PPL_OPT_DELAY', 'actualPosition.value')\n",
    "\n",
    "scan_variable = None if is_dark else scan_variable\n",
    "\n",
    "# index non-DSSC data\n",
    "run = process.load_run_selective(proposal, run_nr, exclude='DSSC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare scan variable and write to file for subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSSC frames per train: 4\n"
     ]
    }
   ],
   "source": [
    "scanfile = './tmp/scan.h5'\n",
    "maskfile = './tmp/mask.h5'\n",
    "\n",
    "for fname in [scanfile, maskfile]:\n",
    "    if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "\n",
    "dssc_info = process.load_dssc_info(proposal, run_nr)\n",
    "fpt = dssc_info['frames_per_train']\n",
    "print('DSSC frames per train:', fpt)\n",
    "\n",
    "scan = process.load_scan_variable(run, scan_variable, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional: discard groups with low number of trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 10\n",
    "\n",
    "grouped = scan.groupby(scan)\n",
    "\n",
    "for val, grp in grouped:\n",
    "    if len(grp) < 10:\n",
    "        scan.loc[{'trainId': grp.trainId.values}] = np.nan\n",
    "\n",
    "scan = scan.dropna('trainId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.to_netcdf(scanfile, group='data', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask individual pulses based on XGM thresholding\n",
    "It is possible to pass a 2d binary mask (DataArray dimensions 'trainId' and 'pulse) to (de)select trains and/or pulses.\n",
    "Broadcasting is supported, so passing just a 1d DataArray with one of the two dimensions is also possible.\n",
    "Care has to be taken to match the frame numbers of the DSSC data - especially when intermediate dark frames are recorded, the number of XGM pulse numbers do not necessarily match the DSSC frame numbers.\n",
    "This functionality may also be used to limit the number of DSSC frames processed, e.g., when more frames than FEL pulses were recorded (take care that dark runs (i.e., possibly without any XGM data) still get a correct mask in that case!).\n",
    "\n",
    "The following example selects based on XGM threshold, but you can of course build your own selection mask based on any other information as well. Just save the final xarray.DataArray to \"maskfile\" as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default mask - all pulses and trains included\n",
    "pulsemask = xr.DataArray(np.ones([len(run.train_ids), fpt], dtype=bool),\n",
    "                         dims=['trainId', 'pulse'],\n",
    "                         coords={'trainId': run.train_ids, 'pulse': range(fpt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASE3 bunches per train: 2\n",
      "rejecting 0 out of 36018 pulses (0.0%) due to xgm threshold\n"
     ]
    }
   ],
   "source": [
    "xgm_threshold = 0\n",
    "xgm_max = np.inf  #1.2e4\n",
    "if not is_dark:\n",
    "    xgm = process.load_xgm(run, print_info=True)\n",
    "    xgm_frame_coords = process.calc_xgm_frame_indices(xgm.shape[1], framepattern)\n",
    "    xgm['pulse'] = xgm_frame_coords\n",
    "    n_frames_dark = len([p for p in framepattern if 'dark' in p])\n",
    "#     n_dssc_frames = xgm.shape[1] * len(framepattern) // n_frames_dark\n",
    "    valid = (xgm > xgm_threshold) * (xgm < xgm_max)\n",
    "    pulsemask = valid.combine_first(pulsemask).astype(bool)\n",
    "    pulsemask = (pulsemask.where(pulsemask)).dropna('trainId', how='any').astype(bool)  # rejects entire train if any pulse < threshold\n",
    "    nrejected = int(valid.size - valid.sum())\n",
    "    percent_rejected = 100 * nrejected / valid.size\n",
    "    print(f'rejecting {nrejected} out of {valid.size} pulses ({percent_rejected:.1f}%) due to xgm threshold')\n",
    "    \n",
    "pulsemask.to_netcdf(maskfile, group='data', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot XGM and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ae85f923dd4b889170662b512f916d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(nrows=2, sharex=True, figsize=[6, 3])\n",
    "\n",
    "# ax1.plot(scan.xgm.mean('dim_0'), label='pumped')\n",
    "ax1.plot(xgm.trainId, xgm, 'o', c='C0', ms=1)\n",
    "if xgm_threshold is not None:\n",
    "    ax1.axhline(xgm_threshold, c='r', lw=1)\n",
    "    ax1.axhline(xgm_max, c='r', lw=1)\n",
    "ax1.set_ylabel('xgm')\n",
    "\n",
    "ax2.plot(scan.trainId, scan)\n",
    "ax2.set_ylabel('scan variable')\n",
    "ax2.set_xlabel('trainId')\n",
    "\n",
    "ax1.set_title(f'run: {run_nr}')\n",
    "\n",
    "tstamp = strftime('%y%m%d_%H%M')\n",
    "# fig.savefig(f'images/run{run_nr}_{tstamp}_scan.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot number of trains per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1254707a3d4b37b0c55bc69a353435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = xr.DataArray(np.ones(len(scan)),\n",
    "                      dims=['scan_variable'],\n",
    "                      coords={'scan_variable': scan.values},\n",
    "                      name='counts')\n",
    "\n",
    "counts = counts.groupby('scan_variable').sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 2.2])\n",
    "ax.plot(counts.scan_variable, counts, 'o', ms=2)\n",
    "ax.set_xlabel('scan variable')\n",
    "ax.set_ylabel('number of trains')\n",
    "ax.set_title(f'run {run_nr}')\n",
    "ax.grid(True)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create joblist for multiprocessing\n",
    "This is a conservative estimate for the maximum number of trains to process simultaneously without using more than \"max_GB\" gigabytes of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 512 trains per chunk\n"
     ]
    }
   ],
   "source": [
    "max_GB = 500\n",
    "\n",
    "# max_GB / (8byte * 16modules * 128px * 512px * N_pulses)\n",
    "chunksize = int(max_GB * 128 // fpt)\n",
    "chunksize = min(512, chunksize)  # more than 512 trains doesn't seem to give any performance benefit\n",
    "print('processing', chunksize, 'trains per chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for m in range(16):\n",
    "    jobs.append(dict(\n",
    "        proposal=proposal,\n",
    "        run_nr=run_nr,\n",
    "        module=m,\n",
    "        chunksize=chunksize,\n",
    "        scanfile=scanfile,\n",
    "        framepattern=framepattern,\n",
    "        maskfile=maskfile,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create multiprocessing pool and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 13:26:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [04:45<00:00,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: 13:32:15\n",
      "CPU times: user 10.3 s, sys: 21.3 s, total: 31.6 s\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "timestamp = time.strftime('%X')\n",
    "print(f'start time: {timestamp}')\n",
    "\n",
    "with multiprocessing.Pool(16) as pool:\n",
    "    module_data = pool.map(process.process_dssc_module, jobs)\n",
    "    \n",
    "print('finished:', time.strftime('%X'))\n",
    "\n",
    "module_data = xr.concat(module_data, dim='module')\n",
    "module_data = module_data.dropna('scan_variable')\n",
    "module_data['run'] = run_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge processed data with scan variable and normalization data\n",
    "If trains/ pulses were filtered, this is a good place to add additional data that has to be filtered in the same way (e.g., TIM). Just use the same binary mask (\"pulsemask\") before grouping and adding it to the processed DSSC data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional: add TIM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = process.load_TIM(run)\n",
    "tim['pulse'] = xgm_frame_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c995b4673647f0bbf6d860c66e7a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(xgm[:, 0], tim[:, 0], 'o', ms=2, label='pumped')\n",
    "ax.plot(xgm[:, 1], tim[:, 1], 'o', ms=2, label='unpumped')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(f'run {run_nr}')\n",
    "ax.set_xlabel('XGM')\n",
    "ax.set_ylabel('TIM (MCP2apd)')\n",
    "\n",
    "tstamp = strftime('%y%m%d_%H%M')\n",
    "# fig.savefig(f'images/run{run_nr}_{tstamp}_TIM.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_dark:\n",
    "    pulses_no_dark = [p for p in framepattern if 'dark' not in p]\n",
    "    \n",
    "    xgm = xgm.where(valid).dropna('trainId', how='any')  # IMPORTANT: use the same mask for normalization data!\n",
    "    xgm = process.split_frames(xgm, pulses_no_dark, prefix='xgm_')\n",
    "    xgm['scan_variable'] = scan\n",
    "    xgm = xgm.groupby('scan_variable').mean('trainId')\n",
    "    module_data = xr.merge([module_data, xgm])\n",
    "    \n",
    "    tim = tim.where(valid).dropna('trainId', how='any')\n",
    "    tim = process.split_frames(tim, pulses_no_dark, prefix='tim_')\n",
    "    tim['scan_variable'] = scan\n",
    "    tim = tim.groupby('scan_variable').mean('trainId')\n",
    "    module_data = xr.merge([module_data, tim])\n",
    "    \n",
    "module_data = module_data.transpose('scan_variable', 'module', 'x', 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving:  ./processed_runs/run132_by-delay.h5\n"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "\n",
    "save_folder = './processed_runs/'\n",
    "\n",
    "if is_dark:\n",
    "    fname = f'run{run_nr}.h5'  # no scan\n",
    "else:\n",
    "    fname = f'run{run_nr}_by-delay.h5'  # run with delay scan (change for other scan types!)\n",
    "\n",
    "\n",
    "save_path = os.path.join(save_folder, fname)\n",
    "file_exists = os.path.isfile(save_path)\n",
    "\n",
    "if (not file_exists) or (file_exists and overwrite):\n",
    "    if file_exists:\n",
    "        os.remove(save_path)\n",
    "    module_data.to_netcdf(save_path, group='data')\n",
    "    os.chmod(os.path.join(save_folder, fname), 664)\n",
    "    print('saving: ', save_path)\n",
    "else:\n",
    "    print('file', save_path, 'exists and overwrite is False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T profile.txt -f process.process_dssc_module process.process_dssc_module(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess_defs_v4 import *\n",
    "job = jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = job['proposal']\n",
    "run_nr = job['run_nr']\n",
    "module = job['module']\n",
    "chunksize = job['chunksize']\n",
    "scanfile = job['scanfile']\n",
    "framepattern = job['framepattern']\n",
    "maskfile = job.get('maskfile', None)\n",
    "\n",
    "sourcename = f'SCS_DET_DSSC1M-1/DET/{module}CH0:xtdf'\n",
    "collection = load_run_selective(proposal, run_nr, include=f'DSSC{module:02d}')\n",
    "ntrains = len(collection.train_ids)\n",
    "    \n",
    "# read preprocessed scan variable from file - selection and (possibly) rounding already done.\n",
    "scan = xr.open_dataarray(scanfile, 'data', autoclose=True, cache=False)\n",
    "# read binary pulse/train mask - e.g. from XGM thresholding\n",
    "if maskfile is not None:\n",
    "    pulsemask = xr.open_dataarray(maskfile, 'data', autoclose=True)\n",
    "    pulsemask = pulsemask.where(pulsemask)  # boolean to [1, NaN]\n",
    "else:\n",
    "    pulsemask = None\n",
    "\n",
    "module_data = prepare_module_empty(scan, framepattern)\n",
    "chunks = np.arange(ntrains, step=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "sel = collection.select_trains(kd.by_index[start_index:start_index + chunksize])\n",
    "data = load_chunk_data(sel, sourcename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pulsemask is not None:\n",
    "    data = (data * pulsemask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_frames(data, framepattern)\n",
    "data['sum_count'] = xr.full_like(data.trainId, fill_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scan_variable'] = scan  # aligns on trainId -> NaN for non-matching trains \n",
    "data = data.groupby('scan_variable').sum('trainId')\n",
    "module_data = merge_chunk_data(module_data, data, framepattern)\n",
    "    \n",
    "\n",
    "# data = process.split_frames(data, framepattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
